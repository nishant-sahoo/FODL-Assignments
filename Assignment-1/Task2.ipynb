{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Task2: Comparison of normalization methods for classification on Image dataset 2\n",
    "# • Model: MLFFNN with 2 hidden layers and tanh activation function\n",
    "# • Lossfunction: Cross-entropy\n",
    "# • Mode of learning: Mini-batch mode\n",
    "# • Stopping criterion: Change in average error below a threshold\n",
    "# • Weight update rule: AdaM\n",
    "# • Normalization method: (1) No normalization, (2) Batch normalization with post-activation normalization \n",
    "# • Use the same value of learning rate parameter\n",
    "# • Use the same initial random values of weights\n",
    "# • For each normalization method, report should include the following: (a) Plot of\n",
    "# average error on training data vs Epoch,(b) Confusion matricesfortraining data and\n",
    "# test data\n",
    "# • Compare number of epochs taken for convergence for normalization methods\n",
    "\n",
    "# Dataset:\n",
    "# it is in ./task2 {folder}\n",
    "# the {folder} has 6 files\n",
    "# test_data.csv\n",
    "# test_label.csv\n",
    "# train_data.csv\n",
    "# train_label.csv\n",
    "# val_data.csv\n",
    "# val_label.csv\n",
    "\n",
    "# Train data shape -> (2000, 36)\n",
    "# Train label shape -> (2000, 1)\n",
    "# Test data shape -> (500, 36)\n",
    "# Test label shape -> (500, 1)\n",
    "# Val data shape -> (500, 36)\n",
    "# Val label shape -> (500, 1)\n",
    "\n",
    "# You can use Pytorch as well. No need to implement neural network from scratch.\n",
    "# You can use any library for plotting the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"\"\n",
    "\n",
    "def seed_all(seed=59+87+122+143):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train_data_path, train_label_path, val_data_path, val_label_path, test_data_path, test_label_path):\n",
    "        self.train_data, self.train_labels = self.load_data(train_data_path, train_label_path)\n",
    "        self.val_data, self.val_labels = self.load_data(val_data_path, val_label_path)\n",
    "        self.test_data, self.test_labels = self.load_data(test_data_path, test_label_path)\n",
    "\n",
    "    def load_data(self, data_path, label_path):\n",
    "        data = pd.read_csv(data_path, header=None).values.astype(np.float32)\n",
    "        labels = pd.read_csv(label_path, header=None).values.flatten().astype(np.int64)\n",
    "        return data, labels\n",
    "\n",
    "    def normalize_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.train_data)\n",
    "        self.train_data = scaler.transform(self.train_data)\n",
    "        self.val_data = scaler.transform(self.val_data)\n",
    "        self.test_data = scaler.transform(self.test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, batch_norm):\n",
    "        super(MLFFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            x = self.activation(self.bn1(self.fc1(x)))\n",
    "            x = self.activation(self.bn2(self.fc2(x)))\n",
    "        else:\n",
    "            x = self.activation(self.fc1(x))\n",
    "            x = self.activation(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, model, criterion, optimizer, epochs, threshold, batch_size):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, train_data, train_labels):\n",
    "        train_errors = []\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "            self.model.train()\n",
    "            for i in range(0, train_data.shape[0], self.batch_size):\n",
    "                data = train_data[i:i + self.batch_size]\n",
    "                labels = train_labels[i:i + self.batch_size]\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_errors.append(loss.item())\n",
    "            if epoch > 0 and abs(train_errors[-1] - train_errors[-2]) < self.threshold:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                break\n",
    "        return train_errors\n",
    "    \n",
    "    def test(self, test_data, test_labels):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(test_data)\n",
    "            loss = self.criterion(outputs, test_labels)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (predicted == test_labels).sum().item() / test_labels.shape[0]\n",
    "        return loss.item(), accuracy, predicted\n",
    "    \n",
    "    def confusion_matrix_train(self, train_data, train_labels):\n",
    "        self.model.eval()\n",
    "        outputs = self.model(train_data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        cm = confusion_matrix(train_labels, predictions.detach().numpy())\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"{self.optimizer_name} Confusion Matrix (Train)\")\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_confusion_matrix_train.png\")\n",
    "        return cm\n",
    "\n",
    "    def confusion_matrix_test(self, test_data, test_labels):\n",
    "        self.model.eval()\n",
    "        outputs = self.model(test_data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        cm = confusion_matrix(test_labels, predictions.detach().numpy())\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"{self.optimizer_name} Confusion Matrix (Test)\")\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_confusion_matrix_test.png\")\n",
    "        return cm\n",
    "\n",
    "    def plot_avg_train_error(self, train_errors):\n",
    "        plt.plot(train_errors)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average Error\")\n",
    "        plt.title(f\"{self.optimizer_name} Average Error on Training Data\")\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_average_error.png\")\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "train_data_path = \"./task2/train_data.csv\"\n",
    "train_label_path = \"./task2/train_label.csv\"\n",
    "val_data_path = \"./task2/val_data.csv\"\n",
    "val_label_path = \"./task2/val_label.csv\"\n",
    "test_data_path = \"./task2/test_data.csv\"\n",
    "test_label_path = \"./task2/test_label.csv\"\n",
    "\n",
    "# Load data\n",
    "data_loader = DataLoader(train_data_path, train_label_path, val_data_path, val_label_path, test_data_path, test_label_path)\n",
    "# data_loader.normalize_data()\n",
    "\n",
    "# Convert data to tensors\n",
    "train_data = torch.tensor(data_loader.train_data)\n",
    "train_labels = torch.tensor(data_loader.train_labels)\n",
    "val_data = torch.tensor(data_loader.val_data)\n",
    "val_labels = torch.tensor(data_loader.val_labels)\n",
    "test_data = torch.tensor(data_loader.test_data)\n",
    "test_labels = torch.tensor(data_loader.test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = data_loader.train_data.shape[1]\n",
    "hidden_size1 = 60\n",
    "hidden_size2 = 30\n",
    "output_size = len(np.unique(data_loader.train_labels))  # Number of unique classes\n",
    "epochs = 1000\n",
    "threshold = 1e-5\n",
    "batch_size = input_size//10\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "# weight_decay = 0.0001\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "folder = f'output/task2/{learning_rate}_{threshold}_{epochs}'\n",
    "os.makedirs(folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 85/1000 [02:37<28:16,  1.85s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold, batch_size)\n\u001b[0;32m      7\u001b[0m classifier\u001b[38;5;241m.\u001b[39moptimizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo_Normalization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss, accuracy, predicted \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mtest(test_data, test_labels)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mNeuralNetworkClassifier.train\u001b[1;34m(self, train_data, train_labels)\u001b[0m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m train_errors\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# No normalization\n",
    "seed_all()\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size, batch_norm=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "classifier = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold, batch_size)\n",
    "classifier.optimizer_name = \"No_Normalization\"\n",
    "train_errors = classifier.train(train_data, train_labels)\n",
    "loss, accuracy, predicted = classifier.test(test_data, test_labels)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "classifier.plot_avg_train_error(train_errors)\n",
    "cm_train = classifier.confusion_matrix_train(train_data, train_labels)\n",
    "cm_test = classifier.confusion_matrix_test(test_data, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "seed_all()\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size, batch_norm=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "classifier = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold, batch_size)\n",
    "classifier.optimizer_name = \"Batch_Normalization\"\n",
    "train_errors = classifier.train(train_data, train_labels)\n",
    "loss, accuracy, predicted = classifier.test(test_data, test_labels)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "classifier.plot_avg_train_error(train_errors)\n",
    "cm_train = classifier.confusion_matrix_train(train_data, train_labels)\n",
    "cm_test = classifier.confusion_matrix_test(test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
