{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Comparison of optimization methods for classification on Image dataset 1\n",
    "# • Model: MLFFNN with 2 hidden layers and tanh activation function\n",
    "# • Lossfunction: Cross-entropy\n",
    "# • Mode of learning: Pattern mode\n",
    "# • Stopping criterion: Change in average error below a threshold\n",
    "# • Weight update rules: (1) Delta rule, (2) Generalized delta rule, (3) AdaGrad, \n",
    "# (4) RMSProp, (5) AdaM\n",
    "# • Use the same value of learning rate parameter\n",
    "# • Use the same initial random values of weights\n",
    "# • For each rule of weight update,reportshould include the following: (a) Plot of average\n",
    "# error on training data vs Epoch, (b) Confusion matricesfor training data and test data\n",
    "# • Compare number of epochs taken for convergence for different update rules.\n",
    "\n",
    "# You can use Pytorch as well. No need to implement neural network from scratch.\n",
    "# You can use any library for plotting the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset:\n",
    "# it is in ./task1 {folder}\n",
    "# the {folder} has 6 files\n",
    "# test_data.csv\n",
    "# test_label.csv\n",
    "# train_data.csv\n",
    "# train_label.csv\n",
    "# val_data.csv\n",
    "# val_label.csv\n",
    "\n",
    "# Train data shape -> (2000, 36)\n",
    "# Train label shape -> (2000, 1)\n",
    "# Test data shape -> (500, 36)\n",
    "# Test label shape -> (500, 1)\n",
    "# Val data shape -> (500, 36)\n",
    "# Val label shape -> (500, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"\"\n",
    "\n",
    "def seed_all(seed=59+87+122+143):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train_data_path, train_label_path, val_data_path, val_label_path, test_data_path, test_label_path):\n",
    "        self.train_data, self.train_labels = self.load_data(train_data_path, train_label_path)\n",
    "        self.val_data, self.val_labels = self.load_data(val_data_path, val_label_path)\n",
    "        self.test_data, self.test_labels = self.load_data(test_data_path, test_label_path)\n",
    "\n",
    "    def load_data(self, data_path, label_path):\n",
    "        data = pd.read_csv(data_path, header=None).values.astype(np.float32)\n",
    "        labels = pd.read_csv(label_path, header=None).values.flatten().astype(np.int64)\n",
    "        return data, labels\n",
    "\n",
    "    def standardize_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.train_data)\n",
    "        self.train_data = scaler.transform(self.train_data)\n",
    "        self.val_data = scaler.transform(self.val_data)\n",
    "        self.test_data = scaler.transform(self.test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLFFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, model, criterion, optimizer, epochs, threshold, batch_size=1):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, train_data, train_labels):\n",
    "        train_errors = []\n",
    "        train_accuracies = []\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "            # shuffle data and labels in unison before each epoch\n",
    "            indices = np.arange(train_data.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            shuf_train_data = train_data[indices]\n",
    "            shuf_train_labels = train_labels[indices]\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in range(0, shuf_train_data.shape[0], self.batch_size):\n",
    "                data = shuf_train_data[i:i + self.batch_size]\n",
    "                labels = shuf_train_labels[i:i + self.batch_size]\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            train_errors.append(epoch_loss / (shuf_train_data.shape[0] / self.batch_size))\n",
    "            train_accuracies.append(correct / total)\n",
    "\n",
    "            if epoch > 0 and abs(train_errors[-1] - train_errors[-2]) < self.threshold:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        return train_errors, train_accuracies\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (predictions == labels).sum().item() / labels.size(0)\n",
    "        return loss.item(), accuracy, predictions\n",
    "    \n",
    "    def confusion_matrix_train(self, train_data, train_labels):\n",
    "        self.model.eval()\n",
    "        outputs = self.model(train_data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        cm = confusion_matrix(train_labels, predictions.detach().numpy())\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        annot_labels = np.array([[f'{norm*100:.2f}%\\n({count})' for norm, count in zip(row_norm, row)] for row_norm, row in zip(cm_norm, cm)])\n",
    "        sns.heatmap(cm_norm, annot=annot_labels, fmt='', cmap=\"YlGnBu\")\n",
    "        plt.xlabel(\"Predicted\", fontsize=14)\n",
    "        plt.ylabel(\"True\", fontsize=14)\n",
    "        plt.title(f\"Confusion Matrix (Train)\", fontsize=18)\n",
    "        plt.axis('equal')\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_confusion_matrix_train.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        return cm\n",
    "\n",
    "    def confusion_matrix_test(self, test_data, test_labels):\n",
    "        self.model.eval()\n",
    "        outputs = self.model(test_data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        cm = confusion_matrix(test_labels, predictions.detach().numpy())\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        annot_labels = np.array([[f'{norm*100:.2f}%\\n({count})' for norm, count in zip(row_norm, row)] for row_norm, row in zip(cm_norm, cm)])\n",
    "        sns.heatmap(cm_norm, annot=annot_labels, fmt='', cmap=\"YlGnBu\")\n",
    "        plt.xlabel(\"Predicted\", fontsize=14)\n",
    "        plt.ylabel(\"True\", fontsize=14)\n",
    "        plt.title(f\"Confusion Matrix (Test)\", fontsize=18)\n",
    "        plt.axis('equal')\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_confusion_matrix_test.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        return cm\n",
    "\n",
    "    def plot_avg_train_error(self, train_errors):\n",
    "        plt.plot(train_errors, color=\"red\")\n",
    "        plt.xlabel(\"Epoch\", fontsize=12)\n",
    "        plt.ylabel(\"Average Error\", fontsize=12)\n",
    "        plt.title(f\"Average Error on Training Data\", fontsize=16)\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_average_error.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_avg_train_accuracy(self, train_accuracies):\n",
    "        plt.plot(train_accuracies, color=\"blue\")\n",
    "        plt.xlabel(\"Epoch\", fontsize=12)\n",
    "        plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "        plt.title(f\"Accuracy on Training Data\", fontsize=16)\n",
    "        plt.savefig(f\"{folder}/{self.optimizer_name}_accuracy.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all(self, train_data, train_labels, test_data, test_labels):\n",
    "        train_errors, train_accuracies = self.train(train_data, train_labels)\n",
    "        loss, accuracy, predictions = self.evaluate(train_data, train_labels)\n",
    "        print(f\"{self.optimizer_name} Train: Loss: {loss}, Accuracy: {accuracy}, Epochs: {len(train_errors)}\")\n",
    "        loss, accuracy, predictions = self.evaluate(test_data, test_labels)\n",
    "        print(f\"{self.optimizer_name} Test: Loss: {loss}, Accuracy: {accuracy}\")\n",
    "        self.plot_avg_train_error(train_errors)\n",
    "        self.plot_avg_train_accuracy(train_accuracies)\n",
    "        self.confusion_matrix_train(train_data, train_labels)\n",
    "        self.confusion_matrix_test(test_data, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "train_data_path = \"./task1/train_data.csv\"\n",
    "train_label_path = \"./task1/train_label.csv\"\n",
    "val_data_path = \"./task1/val_data.csv\"\n",
    "val_label_path = \"./task1/val_label.csv\"\n",
    "test_data_path = \"./task1/test_data.csv\"\n",
    "test_label_path = \"./task1/test_label.csv\"\n",
    "\n",
    "# Load and normalize data\n",
    "data_loader = DataLoader(train_data_path, train_label_path, val_data_path, val_label_path, test_data_path, test_label_path)\n",
    "# data_loader.standardize_data()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_data = torch.tensor(data_loader.train_data)\n",
    "train_labels = torch.tensor(data_loader.train_labels)\n",
    "val_data = torch.tensor(data_loader.val_data)\n",
    "val_labels = torch.tensor(data_loader.val_labels)\n",
    "test_data = torch.tensor(data_loader.test_data)\n",
    "test_labels = torch.tensor(data_loader.test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model parameters\n",
    "# input_size = data_loader.train_data.shape[1]\n",
    "# hidden_size1 = 60\n",
    "# hidden_size2 = 30\n",
    "# output_size = len(np.unique(data_loader.train_labels))  # Number of unique classes\n",
    "# epochs = 10000\n",
    "# learning_rate = 0.01\n",
    "# momentum = 0.9 # for Generalized Delta Rule\n",
    "# threshold = 1e-5\n",
    "\n",
    "# folder = f'output/task1/hyperpar_tuning'\n",
    "# os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code for hyperparameter tuning\n",
    "# learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "# # momentums = [0.5, 0.8, 0.9, 0.95]\n",
    "# hidden_size1s = [50, 60, 70]\n",
    "# hidden_size2s = [25, 30, 35]\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_lr = 0\n",
    "# best_hidden_size1 = 0\n",
    "# best_hidden_size2 = 0\n",
    "\n",
    "# for learning_rate in tqdm(learning_rates, desc=\"Learning Rate\", unit=\"lr\"):\n",
    "#     for hidden_size1 in hidden_size1s:\n",
    "#         for hidden_size2 in hidden_size2s:\n",
    "#             print(f\"Learning Rate: {learning_rate}, Hidden Size 1: {hidden_size1}, Hidden Size 2: {hidden_size2}\")\n",
    "#             seed_all()\n",
    "#             model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#             classifier = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "#             classifier.optimizer_name = \"SGD\"\n",
    "#             train_errors = classifier.train(train_data, train_labels)\n",
    "#             loss, accuracy, predictions = classifier.test(val_data, val_labels)\n",
    "#             print(f\"Validation Accuracy: {accuracy}\")\n",
    "#             if accuracy > best_accuracy:\n",
    "#                 best_accuracy = accuracy\n",
    "#                 best_lr = learning_rate\n",
    "#                 best_hidden_size1 = hidden_size1\n",
    "#                 best_hidden_size2 = hidden_size2\n",
    "#             clear_output(wait=True)\n",
    "\n",
    "# print(f\"Best Accuracy: {best_accuracy}, Best Learning Rate: {best_lr}, Best Hidden Size 1: {best_hidden_size1}, Best Hidden Size 2: {best_hidden_size2}\")\n",
    "# # Best Accuracy: 0.562, Best Learning Rate: 0.1, Best Hidden Size 1: 70, Best Hidden Size 2: 35\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = data_loader.train_data.shape[1]\n",
    "hidden_size1 = 70\n",
    "hidden_size2 = 35\n",
    "output_size = len(np.unique(data_loader.train_labels))  # Number of unique classes\n",
    "epochs = 10000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9 # for Generalized Delta Rule\n",
    "threshold = 1e-5\n",
    "\n",
    "folder = f'output/task1/{learning_rate}_{threshold}_{epochs}_{hidden_size1}_{hidden_size2}'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()\n",
    "# Delta Rule\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "classifier = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "classifier.optimizer_name = \"Delta Rule\"\n",
    "# train_errors, train_accuracies = classifier.train(train_data, train_labels)\n",
    "# loss, accuracy, predictions = classifier.evaluate(test_data, test_labels)\n",
    "# print(f\"Delta Rule: Loss: {loss}, Accuracy: {accuracy}, Epochs: {len(train_errors)}\")\n",
    "# classifier.plot_avg_train_error(train_errors)\n",
    "# classifier.plot_avg_train_accuracy(train_accuracies)\n",
    "# cm_train = classifier.confusion_matrix_train(train_data, train_labels)\n",
    "# cm_test = classifier.confusion_matrix_test(test_data, test_labels)\n",
    "classifier.plot_all(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()\n",
    "# Generalized Delta Rule\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "classifier = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "classifier.optimizer_name = \"Generalized Delta Rule\"\n",
    "classifier.plot_all(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()\n",
    "# AdaGrad\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "trainer = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "trainer.optimizer_name = \"AdaGrad\"\n",
    "trainer.plot_all(train_data, train_labels, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()\n",
    "# RMSProp\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "trainer = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "trainer.optimizer_name = \"RMSProp\"\n",
    "trainer.plot_all(train_data, train_labels, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()\n",
    "# AdaM\n",
    "model = MLFFNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "trainer = NeuralNetworkClassifier(model, criterion, optimizer, epochs, threshold)\n",
    "trainer.optimizer_name = \"AdaM\"\n",
    "trainer.plot_all(train_data, train_labels, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
