{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Image captioning using a CNN with NetVLAD as encoder and a single hidden \n",
    "layer RNN based decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Task 3:** Image captioning using a CNN with NetVLAD as encoder and a single hidden \n",
    "# layer RNN based decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_names_file, captions_file, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.image_captions = self.load_image_captions(image_names_file, captions_file)\n",
    "\n",
    "    def load_image_captions(self, image_names_file, captions_file):\n",
    "        image_captions = {} # Dictionary to store image names and captions\n",
    "        \n",
    "        # Load image names\n",
    "        with open(image_names_file, \"r\") as f_images:\n",
    "            image_names = f_images.read().splitlines()\n",
    "        \n",
    "        # Load captions\n",
    "        with open(captions_file, \"r\") as f_captions:\n",
    "            captions = f_captions.read().splitlines()\n",
    "\n",
    "        for caption in captions:\n",
    "            img_id, img_caption = caption.split(\"\\t\")\n",
    "            img_id = img_id.split(\"#\")[0]  # Extract image ID\n",
    "            if img_id not in image_captions:\n",
    "                image_captions[img_id] = []\n",
    "            image_captions[img_id].append(img_caption)\n",
    "\n",
    "        return [(image_name, image_captions[image_name]) for image_name in image_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, captions = self.image_captions[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.train:\n",
    "            return image, captions\n",
    "        else:\n",
    "            return image, captions[0]  # Return only the first caption for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.special_tokens = [\"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "    def generate_vocabulary(self, captions_file, vocabulary_file):\n",
    "        words = set(self.special_tokens)  # Initialize with special tokens\n",
    "        with open(captions_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                caption = line.strip().split(\"\\t\")[1]\n",
    "                words.update(caption.split())\n",
    "\n",
    "        # Write words to vocabulary file\n",
    "        with open(vocabulary_file, \"w\") as f:\n",
    "            for word in words:\n",
    "                f.write(f\"{word}\\n\")\n",
    "\n",
    "        # Define word_to_index and index_to_word dictionaries\n",
    "        with open(vocabulary_file, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                word = line.strip()\n",
    "                self.word_to_index[word] = i\n",
    "                self.index_to_word[i] = word\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        # Return index of word, or index of <unk> if word not found\n",
    "        return self.word_to_index.get(word, self.word_to_index.get(\"<unk>\"))\n",
    "\n",
    "    def get_index_word(self, index):\n",
    "        return self.index_to_word.get(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NetVLAD layer\n",
    "class NetVLAD(nn.Module):\n",
    "    def __init__(self, num_clusters=2, dim=512, alpha=100.0, normalize_input=True):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.normalize_input = normalize_input\n",
    "        self.conv = nn.Conv2d(dim, num_clusters, kernel_size=(1, 1), bias=True)\n",
    "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.conv.weight = nn.Parameter(\n",
    "            (2.0 * self.alpha * self.centroids).unsqueeze(-1).unsqueeze(-1)\n",
    "        )\n",
    "        self.conv.bias = nn.Parameter(\n",
    "            - self.alpha * self.centroids.norm(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C = x.shape[:2]\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        soft_assign = self.conv(x).view(N, self.num_clusters, -1)\n",
    "        soft_assign = F.softmax(soft_assign, dim=1)\n",
    "\n",
    "        x_flatten = x.view(N, C, -1)\n",
    "        \n",
    "        residual = x_flatten.expand(self.num_clusters, -1, -1, -1).permute(1, 0, 2, 3) - \\\n",
    "            self.centroids.expand(x_flatten.size(-1), -1, -1).permute(1, 2, 0).unsqueeze(0)\n",
    "        residual *= soft_assign.unsqueeze(2)\n",
    "        vlad = residual.sum(dim=-1)\n",
    "\n",
    "        vlad = F.normalize(vlad, p=2, dim=2)\n",
    "        vlad = vlad.view(x.size(0), -1)\n",
    "        vlad = F.normalize(vlad, p=2, dim=1)\n",
    "\n",
    "        return vlad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_clusters=2, dim=512): # k = 2\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-2])\n",
    "        self.net_vlad = NetVLAD(num_clusters=num_clusters, dim=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.net_vlad(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = F.log_softmax(self.out(x[0]), dim=1)\n",
    "        return x, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Image Captioning Model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, target=None, max_length=50):\n",
    "        encoder_output = self.encoder(x)\n",
    "        hidden = self.decoder.initHidden()\n",
    "        loss = 0\n",
    "\n",
    "        if target is not None:\n",
    "            target = torch.tensor(target)  # Convert target to tensor\n",
    "            target = target.unsqueeze(0)   # Add batch dimension\n",
    "\n",
    "            for i in range(target.size(1)):\n",
    "                decoder_output, hidden = self.decoder(target[0, i], hidden)\n",
    "                loss += F.nll_loss(decoder_output, target[0, i])\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, optimizer, word_to_index):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def pad_caption(self, caption, max_length):\n",
    "        caption_indices = [self.word_to_index.get(word, self.word_to_index['<unk>']) for word in caption.split()]\n",
    "        padded_indices = caption_indices + [self.word_to_index['<unk>']] * (max_length - len(caption_indices))\n",
    "        return torch.tensor(padded_indices, device=device)\n",
    "\n",
    "    def train(self, epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i, (images, captions) in enumerate(self.train_loader):\n",
    "                images = images.to(device)\n",
    "                # Determine the maximum caption length for padding\n",
    "                max_caption_length = max(len(caption.split()) for caption_tuple in captions for caption in caption_tuple)\n",
    "                captions_tensor = []\n",
    "\n",
    "                # Process and pad each caption in the batch\n",
    "                for caption_tuple in captions:\n",
    "                    for caption in caption_tuple:\n",
    "                        caption_tensor = self.pad_caption(caption, max_caption_length)\n",
    "                        captions_tensor.append(caption_tensor)\n",
    "\n",
    "                # Stack all caption tensors\n",
    "                captions_tensor = torch.stack(captions_tensor).to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model(images, captions_tensor)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(self.train_loader)}], Loss: {loss.item():.4f}\")\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {total_loss/len(self.train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, word_to_index, index_to_word):\n",
    "        self.model = model.to(device)\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "\n",
    "    def evaluate_bleu(self, reference_caption, generated_caption):\n",
    "        reference_caption = reference_caption.split()\n",
    "        generated_caption = generated_caption.split()\n",
    "\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        bleu_1 = sentence_bleu([reference_caption], generated_caption, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu_2 = sentence_bleu([reference_caption], generated_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "        bleu_3 = sentence_bleu([reference_caption], generated_caption, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "        bleu_4 = sentence_bleu([reference_caption], generated_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "        return bleu_1, bleu_2, bleu_3, bleu_4\n",
    "\n",
    "    def evaluate_model(self, dataset):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_bleu_1 = 0\n",
    "        total_bleu_2 = 0\n",
    "        total_bleu_3 = 0\n",
    "        total_bleu_4 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(dataset)):\n",
    "                image, caption = dataset[i]\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                caption = caption.split(\" \")\n",
    "                target = torch.tensor([self.word_to_index[word] for word in caption]).to(device)\n",
    "\n",
    "                loss = self.model(image, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Generate a caption\n",
    "                hidden = self.model.decoder.initHidden(device)\n",
    "                word = torch.tensor([self.word_to_index[\"<start>\"]]).to(device)\n",
    "                words = []\n",
    "\n",
    "                for i in range(50):\n",
    "                    output, hidden = self.model.decoder(word, hidden)\n",
    "                    word_index = output.argmax().item()\n",
    "                    word = torch.tensor([word_index]).to(device)\n",
    "                    words.append(self.index_to_word[word_index])\n",
    "\n",
    "                    if self.index_to_word[word_index] == \"<end>\":\n",
    "                        break\n",
    "\n",
    "                generated_caption = \" \".join(words)\n",
    "                bleu_1, bleu_2, bleu_3, bleu_4 = self.evaluate_bleu(caption, generated_caption)\n",
    "                total_bleu_1 += bleu_1\n",
    "                total_bleu_2 += bleu_2\n",
    "                total_bleu_3 += bleu_3\n",
    "                total_bleu_4 += bleu_4\n",
    "\n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            avg_bleu_1 = total_bleu_1 / len(dataset)\n",
    "            avg_bleu_2 = total_bleu_2 / len(dataset)\n",
    "            avg_bleu_3 = total_bleu_3 / len(dataset)\n",
    "            avg_bleu_4 = total_bleu_4 / len(dataset)\n",
    "\n",
    "            print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Average BLEU@1: {avg_bleu_1:.4f}\")\n",
    "            print(f\"Average BLEU@2: {avg_bleu_2:.4f}\")\n",
    "            print(f\"Average BLEU@3: {avg_bleu_3:.4f}\")\n",
    "            print(f\"Average BLEU@4: {avg_bleu_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define dataset and dataloader for training\n",
    "train_dataset = CaptioningDataset(\n",
    "    root_dir=\"./dataset/captioning/Images\",\n",
    "    image_names_file=\"./dataset/captioning/image_names.txt\",\n",
    "    captions_file=\"./dataset/captioning/captions.txt\",\n",
    "    transform=data_transform,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define dataset and dataloader for testing\n",
    "test_dataset = CaptioningDataset(\n",
    "    root_dir=\"./dataset/captioning/Images\",\n",
    "    image_names_file=\"./dataset/captioning/image_names.txt\",\n",
    "    captions_file=\"./dataset/captioning/captions.txt\",\n",
    "    transform=data_transform,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Show an image with its caption from the training dataset\n",
    "image, captions = train_dataset[100]\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Captions:\", captions)\n",
    "\n",
    "# Show an image with its caption from the testing dataset\n",
    "image, caption = test_dataset[100]\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary()\n",
    "vocabulary.generate_vocabulary(\n",
    "    captions_file=\"./dataset/captioning/captions.txt\",\n",
    "    vocabulary_file=\"./dataset/captioning/vocabulary.txt\"\n",
    ")\n",
    "\n",
    "word_to_index = vocabulary.word_to_index\n",
    "index_to_word = vocabulary.index_to_word\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index[\"<start>\"]\n",
    "index_to_word[2939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print output size of the encoder\n",
    "encoder = Encoder(num_clusters=2, dim=512)\n",
    "encoder_output = encoder(image.unsqueeze(0))\n",
    "print(encoder_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_caption(caption, max_length, word_to_index, device):\n",
    "#     caption_indices = [word_to_index.get(word, word_to_index['<unk>']) for word in caption.split()]\n",
    "#     padded_indices = caption_indices + [word_to_index['<unk>']] * (max_length - len(caption_indices))\n",
    "#     return torch.tensor(padded_indices).to(device)\n",
    "\n",
    "# def train(train_loader, word_to_index, device):\n",
    "#     for i, (images, captions) in enumerate(train_loader):\n",
    "#         images = images.to(device)\n",
    "#         # Calculate the maximum length of captions in the batch\n",
    "#         max_caption_length = max(len(caption.split()) for caption_tuple in captions for caption in caption_tuple)\n",
    "#         captions_tensor = []\n",
    "\n",
    "#         for caption_tuple in captions:\n",
    "#             for caption in caption_tuple:\n",
    "#                 caption_tensor = pad_caption(caption, max_caption_length, word_to_index, device)\n",
    "#                 captions_tensor.append(caption_tensor)\n",
    "\n",
    "#         captions_tensor = torch.stack(captions_tensor)  # Stack all caption tensors\n",
    "#         captions_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (images, captions) in enumerate(train_loader):\n",
    "#     print(images.size())\n",
    "#     images.to(device)\n",
    "#     # captions is a list of 5 tuples\n",
    "#     # each tuple is a tuple of 32 captions\n",
    "#     # caption[j][k] is a string\n",
    "\n",
    "#     # I want to put them to device\n",
    "#     # it has to be a pytorch tensor\n",
    "#     captions_tensor = []\n",
    "#     for j in range(5):\n",
    "#         tuple_captions = captions[j]  # Get tuple of 32 captions\n",
    "#         tuple_caption_tensors = []\n",
    "#         for caption in tuple_captions:\n",
    "#             caption_indices = [word_to_index.get(word, word_to_index['<unk>']) for word in caption.split()]\n",
    "#             caption_tensor = torch.tensor(caption_indices).unsqueeze(0).to(device)\n",
    "#             tuple_caption_tensors.append(caption_tensor)\n",
    "#         captions_tensor.append(tuple_caption_tensors)\n",
    "#     print(len(captions_tensor))\n",
    "\n",
    "#     # print(type(captions))\n",
    "#     # print(len(captions[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "encoder = Encoder(num_clusters=2, dim=512)\n",
    "decoder = Decoder(input_size=encoder.net_vlad.dim * encoder.net_vlad.num_clusters, hidden_size=256, vocab_size=vocab_size)\n",
    "model = ImageCaptioningModel(encoder, decoder)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create Trainer instance and train the model\n",
    "trainer = Trainer(model, train_loader, optimizer, word_to_index)\n",
    "trainer.train(epochs=10)\n",
    "\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"image_captioning_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = ImageCaptioningModel(encoder, decoder)\n",
    "model.load_state_dict(torch.load(\"image_captioning_model.pth\"))\n",
    "\n",
    "# Create the evaluator instance\n",
    "evaluator = Evaluator(model, word_to_index, index_to_word)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator.evaluate_model(train_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
